{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Adam', ',', 'how', 'are', 'you', '?', 'I', 'hope', 'everything', 'is', 'going', 'well', '.', 'Today', 'is', 'a', 'good', 'day', ',', 'see', 'you', 'dude', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "mytext = 'Hello Mr. Adam, how are you? I hope everything is going well. Today is a good day, see you dude.'\n",
    "print(word_tokenize(mytext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shlok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEXT MINING\n",
    "def preProcess(dataframe):\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))    \n",
    "    names=dataframe['RESTAURANT_NAME'].values\n",
    "\n",
    "    #Using Regex functions to remove non-essential characters\n",
    "    t1=[]\n",
    "    for i in range(len(names)):\n",
    "        t1.append(re.sub('[^a-zA-Z]+',' ',str(names[i])))\n",
    "    for i in range(len(t1)):\n",
    "        t1[i] = ' '.join(word for word in t1[i].split() if word not in stop_words)\n",
    "        t1[i]=t1[i].lower()\n",
    "        \n",
    "    corpus_stemmed = []\n",
    "    \n",
    "    #Lementing words\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    for d in t1:\n",
    "        words = pd.Series(wordpunct_tokenize(d),dtype='object')\n",
    "        stemmed_words = words.apply(wordnet_lemmatizer.lemmatize)\n",
    "        corpus_stemmed.append(' '.join(list(stemmed_words)))\n",
    "    \n",
    "    return corpus_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\shlok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NNP'), ('Mr.', 'NNP'), ('Adam', 'NNP'), ('how', 'WRB'), ('are', 'VBP'), ('you', 'PRP'), ('I', 'PRP'), ('hope', 'VBP'), ('everything', 'NN'), ('is', 'VBZ'), ('going', 'VBG'), ('well', 'RB'), ('Today', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('good', 'JJ'), ('day', 'NN'), ('see', 'VBP'), ('you', 'PRP'), ('dude', 'VBP')]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "result = TextBlob(mytext)\n",
    "print(result.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from textblob) (3.5)\n",
      "Requirement already satisfied: click in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (7.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.50.2)\n",
      "Requirement already satisfied: regex in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2020.10.15)\n",
      "Requirement already satisfied: joblib in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (0.17.0)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting h5\n",
      "  Downloading h5-0.5.0-py3-none-any.whl (4.2 kB)\n",
      "Collecting h5py<4.0.0,>=3.2.0\n",
      "  Downloading h5py-3.6.0-cp38-cp38-win_amd64.whl (2.8 MB)\n",
      "Requirement already satisfied: numpy!=1.19.4,>=1.7 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from h5) (1.19.2)\n",
      "Installing collected packages: h5py, h5\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.1.0\n",
      "    Uninstalling h5py-3.1.0:\n",
      "      Successfully uninstalled h5py-3.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\shlok\\\\AppData\\\\Local\\\\Temp\\\\pip-uninstall-l4ez9ir8\\\\h5.cp38-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-splitter\n",
      "  Downloading sentence_splitter-1.4-py2.py3-none-any.whl (44 kB)\n",
      "Requirement already satisfied: regex>=2017.12.12 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from sentence-splitter) (2020.10.15)\n",
      "Installing collected packages: sentence-splitter\n",
      "Successfully installed sentence-splitter-1.4\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from transformers) (4.50.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from transformers) (5.3.1)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from transformers) (2.24.0)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp38-cp38-win_amd64.whl (2.0 MB)\n",
      "Requirement already satisfied: filelock in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from transformers) (2020.10.15)\n",
      "Collecting packaging>=20.0\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (0.17.0)\n",
      "Requirement already satisfied: six in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Installing collected packages: packaging, tokenizers, sacremoses, huggingface-hub, transformers\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 20.4\n",
      "    Uninstalling packaging-20.4:\n",
      "      Successfully uninstalled packaging-20.4\n",
      "Successfully installed huggingface-hub-0.1.2 packaging-21.3 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n",
      "Collecting SentencePiece\n",
      "  Downloading sentencepiece-0.1.96-cp38-cp38-win_amd64.whl (1.1 MB)\n",
      "Installing collected packages: SentencePiece\n",
      "Successfully installed SentencePiece-0.1.96\n"
     ]
    }
   ],
   "source": [
    "#Installing dependencies\n",
    "!pip install sentence-splitter\n",
    "!pip install transformers\n",
    "!pip install SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the PEGASUS Transformer model\n",
    "import torch\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "model_name = 'tuner007/pegasus_paraphrase'\n",
    "torch_device = 'cpu'#'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "def get_response(input_text,num_return_sequences,num_beams):\n",
    "    batch = tokenizer([input_text],truncation=True,padding='longest',max_length=60, return_tensors=\"pt\").to(torch_device)\n",
    "    translated = model.generate(**batch,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "    tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    return tgt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Is the system required to return information about cities?',\n",
       " 'Is it necessary for the system to return information about cities?',\n",
       " 'Is the system required to return information about the cities?',\n",
       " 'Is it necessary for the system to return information about the cities?',\n",
       " 'Does the system need to return information about the cities?',\n",
       " 'Does the system need to give information about the cities?',\n",
       " 'Does the system need to know about the cities?',\n",
       " 'Do the system need to return information about the cities?',\n",
       " 'Does the system need to return information about cities?',\n",
       " 'Is the system needed to return information about cities?']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test input sentence\n",
    "text = \"Does the system need to return information about cities?\"\n",
    "\n",
    "#printing response\n",
    "get_response(text, 10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_lightning\n",
      "  Downloading pytorch_lightning-1.5.2-py3-none-any.whl (1.0 MB)\n",
      "Requirement already satisfied: future>=0.17.1 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from pytorch_lightning) (0.18.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from pytorch_lightning) (3.7.4.3)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from pytorch_lightning) (4.50.2)\n",
      "Requirement already satisfied: torch>=1.6 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from pytorch_lightning) (1.8.1)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from pytorch_lightning) (2.6.0)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from pytorch_lightning) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from pytorch_lightning) (1.19.2)\n",
      "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
      "  Downloading fsspec-2021.11.0-py3-none-any.whl (132 kB)\n",
      "Collecting pyDeprecate==0.3.1\n",
      "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
      "Collecting torchmetrics>=0.4.1\n",
      "  Downloading torchmetrics-0.6.0-py3-none-any.whl (329 kB)\n",
      "Requirement already satisfied: PyYAML>=5.1 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from pytorch_lightning) (5.3.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.7.4)\n",
      "Requirement already satisfied: requests in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.24.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from packaging>=17.0->pytorch_lightning) (2.4.7)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.13.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.40.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.35.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (50.3.1.post20201107)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.18.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.35.1)\n",
      "Requirement already satisfied: six in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch_lightning) (1.15.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2021.10.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.10)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (5.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (20.3.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.6.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\shlok\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.0)\n",
      "Installing collected packages: fsspec, torchmetrics, pyDeprecate, pytorch-lightning\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 0.8.3\n",
      "    Uninstalling fsspec-0.8.3:\n",
      "      Successfully uninstalled fsspec-0.8.3\n",
      "Successfully installed fsspec-2021.11.0 pyDeprecate-0.3.1 pytorch-lightning-1.5.2 torchmetrics-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvidia-smi' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shlok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5FineTuner(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super(T5FineTuner, self).__init__()\n",
    "        self.hparams = hparams\n",
    "\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n",
    "\n",
    "    def is_logger(self):\n",
    "        return True #self.trainer.proc_rank <= 0\n",
    "\n",
    "    def forward(\n",
    "            self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, lm_labels=None\n",
    "    ):\n",
    "        return self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            lm_labels=lm_labels,\n",
    "        )\n",
    "\n",
    "    def _step(self, batch):\n",
    "        lm_labels = batch[\"target_ids\"]\n",
    "        lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        outputs = self(\n",
    "            input_ids=batch[\"source_ids\"],\n",
    "            attention_mask=batch[\"source_mask\"],\n",
    "            lm_labels=lm_labels,\n",
    "            decoder_attention_mask=batch['target_mask']\n",
    "        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "\n",
    "        tensorboard_logs = {\"train_loss\": loss}\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n",
    "        return {\"avg_train_loss\": avg_train_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "        return {\"val_loss\": loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        tensorboard_logs = {\"val_loss\": avg_loss}\n",
    "        return {\"avg_val_loss\": avg_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
    "\n",
    "        model = self.model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
    "        self.opt = optimizer\n",
    "        return [optimizer]\n",
    "\n",
    "    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None, on_tpu=False, using_native_amp=False, using_lbfgs=False):\n",
    "        if self.trainer.use_tpu:\n",
    "            xm.optimizer_step(optimizer)\n",
    "        else:\n",
    "            optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        self.lr_scheduler.step()\n",
    "\n",
    "    def get_tqdm_dict(self):\n",
    "        tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
    "\n",
    "        return tqdm_dict\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"train\", args=self.hparams)\n",
    "        dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True,\n",
    "                                num_workers=4)\n",
    "        t_total = (\n",
    "                (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n",
    "                // self.hparams.gradient_accumulation_steps\n",
    "                * float(self.hparams.num_train_epochs)\n",
    "        )\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n",
    "        )\n",
    "        self.lr_scheduler = scheduler\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"dev\", args=self.hparams)\n",
    "        return DataLoader(val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LoggingCallback(pl.Callback):\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        logger.info(\"***** Validation results *****\")\n",
    "        if pl_module.is_logger():\n",
    "            metrics = trainer.callback_metrics\n",
    "            # Log results\n",
    "            for key in sorted(metrics):\n",
    "                if key not in [\"log\", \"progress_bar\"]:\n",
    "                    logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "\n",
    "    def on_test_end(self, trainer, pl_module):\n",
    "        logger.info(\"***** Test results *****\")\n",
    "\n",
    "        if pl_module.is_logger():\n",
    "            metrics = trainer.callback_metrics\n",
    "\n",
    "      # Log and save results to file\n",
    "            output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n",
    "            with open(output_test_results_file, \"w\") as writer:\n",
    "                for key in sorted(metrics):\n",
    "                    if key not in [\"log\", \"progress_bar\"]:\n",
    "                        logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "                        writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file data already exists.\n",
      "--2021-11-20 12:20:15--  https://storage.googleapis.com/paws/english/paws_wiki_labeled_final.tar.gz\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 2607:f8b0:4009:806::2010, 2607:f8b0:4009:817::2010, 2607:f8b0:4009:818::2010, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|2607:f8b0:4009:806::2010|:443... connected.\n",
      "WARNING: cannot verify storage.googleapis.com's certificate, issued by 'CN=GTS CA 1C3,O=Google Trust Services LLC,C=US':\n",
      "  Unable to locally verify the issuer's authority.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4687157 (4.5M) [application/gzip]\n",
      "Saving to: 'data/paws_wiki_labeled_final.tar.gz.2'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  1%  931K 5s\n",
      "    50K .......... .......... .......... .......... ..........  2% 1.63M 4s\n",
      "   100K .......... .......... .......... .......... ..........  3% 2.55M 3s\n",
      "   150K .......... .......... .......... .......... ..........  4% 3.82M 3s\n",
      "   200K .......... .......... .......... .......... ..........  5% 4.02M 2s\n",
      "   250K .......... .......... .......... .......... ..........  6% 4.47M 2s\n",
      "   300K .......... .......... .......... .......... ..........  7% 3.20M 2s\n",
      "   350K .......... .......... .......... .......... ..........  8% 4.19M 2s\n",
      "   400K .......... .......... .......... .......... ..........  9% 3.89M 2s\n",
      "   450K .......... .......... .......... .......... .......... 10% 2.72M 2s\n",
      "   500K .......... .......... .......... .......... .......... 12% 4.19M 2s\n",
      "   550K .......... .......... .......... .......... .......... 13% 3.72M 1s\n",
      "   600K .......... .......... .......... .......... .......... 14% 4.12M 1s\n",
      "   650K .......... .......... .......... .......... .......... 15% 3.97M 1s\n",
      "   700K .......... .......... .......... .......... .......... 16% 4.23M 1s\n",
      "   750K .......... .......... .......... .......... .......... 17% 4.29M 1s\n",
      "   800K .......... .......... .......... .......... .......... 18% 3.94M 1s\n",
      "   850K .......... .......... .......... .......... .......... 19% 3.86M 1s\n",
      "   900K .......... .......... .......... .......... .......... 20% 4.30M 1s\n",
      "   950K .......... .......... .......... .......... .......... 21% 4.15M 1s\n",
      "  1000K .......... .......... .......... .......... .......... 22% 6.57M 1s\n",
      "  1050K .......... .......... .......... .......... .......... 24% 29.0M 1s\n",
      "  1100K .......... .......... .......... .......... .......... 25% 38.9M 1s\n",
      "  1150K .......... .......... .......... .......... .......... 26% 33.8M 1s\n",
      "  1200K .......... .......... .......... .......... .......... 27% 30.6M 1s\n",
      "  1250K .......... .......... .......... .......... .......... 28% 40.9M 1s\n",
      "  1300K .......... .......... .......... .......... .......... 29% 31.2M 1s\n",
      "  1350K .......... .......... .......... .......... .......... 30% 22.9M 1s\n",
      "  1400K .......... .......... .......... .......... .......... 31% 31.8M 1s\n",
      "  1450K .......... .......... .......... .......... .......... 32% 30.5M 1s\n",
      "  1500K .......... .......... .......... .......... .......... 33% 19.8M 1s\n",
      "  1550K .......... .......... .......... .......... .......... 34% 24.9M 1s\n",
      "  1600K .......... .......... .......... .......... .......... 36% 28.7M 1s\n",
      "  1650K .......... .......... .......... .......... .......... 37% 40.8M 1s\n",
      "  1700K .......... .......... .......... .......... .......... 38% 37.6M 1s\n",
      "  1750K .......... .......... .......... .......... .......... 39% 41.7M 1s\n",
      "  1800K .......... .......... .......... .......... .......... 40% 28.1M 1s\n",
      "  1850K .......... .......... .......... .......... .......... 41% 28.6M 0s\n",
      "  1900K .......... .......... .......... .......... .......... 42% 40.0M 0s\n",
      "  1950K .......... .......... .......... .......... .......... 43% 39.4M 0s\n",
      "  2000K .......... .......... .......... .......... .......... 44% 20.4M 0s\n",
      "  2050K .......... .......... .......... .......... .......... 45% 31.6M 0s\n",
      "  2100K .......... .......... .......... .......... .......... 46% 33.5M 0s\n",
      "  2150K .......... .......... .......... .......... .......... 48% 31.9M 0s\n",
      "  2200K .......... .......... .......... .......... .......... 49% 29.1M 0s\n",
      "  2250K .......... .......... .......... .......... .......... 50% 28.5M 0s\n",
      "  2300K .......... .......... .......... .......... .......... 51% 24.7M 0s\n",
      "  2350K .......... .......... .......... .......... .......... 52% 32.6M 0s\n",
      "  2400K .......... .......... .......... .......... .......... 53% 32.9M 0s\n",
      "  2450K .......... .......... .......... .......... .......... 54% 36.6M 0s\n",
      "  2500K .......... .......... .......... .......... .......... 55% 39.0M 0s\n",
      "  2550K .......... .......... .......... .......... .......... 56% 20.3M 0s\n",
      "  2600K .......... .......... .......... .......... .......... 57% 29.6M 0s\n",
      "  2650K .......... .......... .......... .......... .......... 58% 39.5M 0s\n",
      "  2700K .......... .......... .......... .......... .......... 60% 40.2M 0s\n",
      "  2750K .......... .......... .......... .......... .......... 61% 33.1M 0s\n",
      "  2800K .......... .......... .......... .......... .......... 62% 25.1M 0s\n",
      "  2850K .......... .......... .......... .......... .......... 63% 30.8M 0s\n",
      "  2900K .......... .......... .......... .......... .......... 64% 38.4M 0s\n",
      "  2950K .......... .......... .......... .......... .......... 65% 30.1M 0s\n",
      "  3000K .......... .......... .......... .......... .......... 66% 36.0M 0s\n",
      "  3050K .......... .......... .......... .......... .......... 67% 18.0M 0s\n",
      "  3100K .......... .......... .......... .......... .......... 68% 32.2M 0s\n",
      "  3150K .......... .......... .......... .......... .......... 69% 37.7M 0s\n",
      "  3200K .......... .......... .......... .......... .......... 71% 32.2M 0s\n",
      "  3250K .......... .......... .......... .......... .......... 72% 34.8M 0s\n",
      "  3300K .......... .......... .......... .......... .......... 73% 37.2M 0s\n",
      "  3350K .......... .......... .......... .......... .......... 74% 27.4M 0s\n",
      "  3400K .......... .......... .......... .......... .......... 75% 26.6M 0s\n",
      "  3450K .......... .......... .......... .......... .......... 76% 39.1M 0s\n",
      "  3500K .......... .......... .......... .......... .......... 77% 39.4M 0s\n",
      "  3550K .......... .......... .......... .......... .......... 78% 32.5M 0s\n",
      "  3600K .......... .......... .......... .......... .......... 79% 21.1M 0s\n",
      "  3650K .......... .......... .......... .......... .......... 80% 36.9M 0s\n",
      "  3700K .......... .......... .......... .......... .......... 81% 34.3M 0s\n",
      "  3750K .......... .......... .......... .......... .......... 83% 33.7M 0s\n",
      "  3800K .......... .......... .......... .......... .......... 84% 27.6M 0s\n",
      "  3850K .......... .......... .......... .......... .......... 85% 30.7M 0s\n",
      "  3900K .......... .......... .......... .......... .......... 86% 36.8M 0s\n",
      "  3950K .......... .......... .......... .......... .......... 87% 42.1M 0s\n",
      "  4000K .......... .......... .......... .......... .......... 88% 29.7M 0s\n",
      "  4050K .......... .......... .......... .......... .......... 89% 33.4M 0s\n",
      "  4100K .......... .......... .......... .......... .......... 90% 34.2M 0s\n",
      "  4150K .......... .......... .......... .......... .......... 91% 38.4M 0s\n",
      "  4200K .......... .......... .......... .......... .......... 92% 36.3M 0s\n",
      "  4250K .......... .......... .......... .......... .......... 93% 27.2M 0s\n",
      "  4300K .......... .......... .......... .......... .......... 95% 39.8M 0s\n",
      "  4350K .......... .......... .......... .......... .......... 96% 40.6M 0s\n",
      "  4400K .......... .......... .......... .......... .......... 97% 26.7M 0s\n",
      "  4450K .......... .......... .......... .......... .......... 98% 27.7M 0s\n",
      "  4500K .......... .......... .......... .......... .......... 99% 38.8M 0s\n",
      "  4550K .......... .......... .......                         100% 34.8M=0.4s\n",
      "\n",
      "2021-11-20 12:20:15 (10.3 MB/s) - 'data/paws_wiki_labeled_final.tar.gz.2' saved [4687157/4687157]\n",
      "\n",
      "x final/test.tsv\n",
      "x final/\n",
      "x final/train.tsv\n",
      "x final/dev.tsv\n",
      "'mv' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!wget https://storage.googleapis.com/paws/english/paws_wiki_labeled_final.tar.gz -P data --no-check-certificate\n",
    "!tar -xvf data/paws_wiki_labeled_final.tar.gz -C data\n",
    "!mv data/final/* data\n",
    "!rm -r data/final\n",
    "!rm -r data/paws_wiki_labeled_final.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>In Paris , in October 1560 , he secretly met t...</td>\n",
       "      <td>In October 1560 , he secretly met with the Eng...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The NBA season of 1975 -- 76 was the 30th seas...</td>\n",
       "      <td>The 1975 -- 76 season of the National Basketba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>There are also specific discussions , public p...</td>\n",
       "      <td>There are also public discussions , profile sp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>When comparable rates of flow can be maintaine...</td>\n",
       "      <td>The results are high when comparable flow rate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>It is the seat of Zerendi District in Akmola R...</td>\n",
       "      <td>It is the seat of the district of Zerendi in A...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                          sentence1  \\\n",
       "0   1  In Paris , in October 1560 , he secretly met t...   \n",
       "1   2  The NBA season of 1975 -- 76 was the 30th seas...   \n",
       "2   3  There are also specific discussions , public p...   \n",
       "3   4  When comparable rates of flow can be maintaine...   \n",
       "4   5  It is the seat of Zerendi District in Akmola R...   \n",
       "\n",
       "                                           sentence2  label  \n",
       "0  In October 1560 , he secretly met with the Eng...      0  \n",
       "1  The 1975 -- 76 season of the National Basketba...      1  \n",
       "2  There are also public discussions , profile sp...      0  \n",
       "3  The results are high when comparable flow rate...      1  \n",
       "4  It is the seat of the district of Zerendi in A...      1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv(\"data/train.tsv\", sep=\"\\t\")#.astype(str)\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  id                                          sentence1  \\\n",
      "0  1  In Paris , in October 1560 , he secretly met t...   \n",
      "1  2  The NBA season of 1975 -- 76 was the 30th seas...   \n",
      "2  3  There are also specific discussions , public p...   \n",
      "3  4  When comparable rates of flow can be maintaine...   \n",
      "4  5  It is the seat of Zerendi District in Akmola R...   \n",
      "\n",
      "                                           sentence2 label  \n",
      "0  In October 1560 , he secretly met with the Eng...     0  \n",
      "1  The 1975 -- 76 season of the National Basketba...     1  \n",
      "2  There are also public discussions , profile sp...     0  \n",
      "3  The results are high when comparable flow rate...     1  \n",
      "4  It is the seat of the district of Zerendi in A...     1  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a22ded2fd547d488df69f20eeb3436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=791656.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214c55d144b742ecabd318a0cc441b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1389353.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ecef6e7a5cf4ccbb557f42f2e612c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "args_dict = dict(\n",
    "    data_dir=\"\", # path for data files\n",
    "    output_dir=\"\", # path to save the checkpoints\n",
    "    model_name_or_path='t5-large',\n",
    "    tokenizer_name_or_path='t5-large',\n",
    "    max_seq_length=256,\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.0,\n",
    "    adam_epsilon=1e-8,\n",
    "    warmup_steps=0,\n",
    "    train_batch_size=1,\n",
    "    eval_batch_size=1,\n",
    "    num_train_epochs=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    n_gpu=1,\n",
    "    early_stop_callback=False,\n",
    "    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n",
    "    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n",
    "    max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "train_path = \"data/train.tsv\"\n",
    "val_path = \"data/dev.tsv\"\n",
    "\n",
    "train = pd.read_csv(train_path, sep=\"\\t\").astype(str)\n",
    "print(train.head())\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParaphraseDataset(Dataset):\n",
    "    def __init__(self, tokenizer, data_dir, type_path, max_len=512):\n",
    "        self.path = os.path.join(data_dir, type_path + '.tsv')\n",
    "\n",
    "        self.source_column = \"sentence1\"\n",
    "        self.target_column = \"sentence2\"\n",
    "        self.data = pd.read_csv(self.path, sep=\"\\t\").astype(str)\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "\n",
    "        self._build()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
    "        target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
    "\n",
    "        src_mask = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "        target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "\n",
    "        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n",
    "\n",
    "    def _build(self):\n",
    "        for idx in range(len(self.data)):\n",
    "            input_, target = self.data.loc[idx, self.source_column], self.data.loc[idx, self.target_column]\n",
    "\n",
    "            input_ = \"paraphrase: \"+ input_ + ' </s>'\n",
    "            target = target + \" </s>\"\n",
    "\n",
    "            # tokenize inputs\n",
    "            tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
    "                [input_], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\", truncation='longest_first'\n",
    "            )\n",
    "            # tokenize targets\n",
    "            tokenized_targets = self.tokenizer.batch_encode_plus(\n",
    "                [target], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\", truncation='longest_first'\n",
    "            )\n",
    "\n",
    "            self.inputs.append(tokenized_inputs)\n",
    "            self.targets.append(tokenized_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shlok\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2212: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "C:\\Users\\shlok\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:190: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val dataset:  8000\n",
      "paraphrase: Lucas Dumbrell Motorsport replaced Aaren Russell with Alex Davison for this event and the following.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Lucas Dumbrell Motorsport replaced Alex Davison with Aaren Russell for this and the following event.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "{'data_dir': 'data', 'output_dir': 't5_paraphrase', 'model_name_or_path': 't5-large', 'tokenizer_name_or_path': 't5-large', 'max_seq_length': 256, 'learning_rate': 0.0003, 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'warmup_steps': 0, 'train_batch_size': 1, 'eval_batch_size': 1, 'num_train_epochs': 10, 'gradient_accumulation_steps': 16, 'n_gpu': 1, 'early_stop_callback': False, 'fp_16': False, 'opt_level': 'O1', 'max_grad_norm': 1.0, 'seed': 42}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'filepath'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-87206c206425>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m checkpoint_callback = pl.callbacks.ModelCheckpoint(\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mfilepath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"checkpoint\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"val_loss\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"min\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_top_k\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'filepath'"
     ]
    }
   ],
   "source": [
    "dataset = ParaphraseDataset(tokenizer, 'data', 'dev', 256)\n",
    "print(\"Val dataset: \",len(dataset))\n",
    "\n",
    "data = dataset[61]\n",
    "print(tokenizer.decode(data['source_ids']))\n",
    "print(tokenizer.decode(data['target_ids']))\n",
    "\n",
    "if not os.path.exists('t5_paraphrase'):\n",
    "    os.makedirs('t5_paraphrase')\n",
    "\n",
    "args_dict.update({'data_dir': 'data', 'output_dir': 't5_paraphrase', 'num_train_epochs':10,'max_seq_length':256})\n",
    "args = argparse.Namespace(**args_dict)\n",
    "print(args_dict)\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(filepath=args.output_dir, \n",
    "                                                   prefix=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=5)\n",
    "\n",
    "train_params = dict(\n",
    "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "    gpus=args.n_gpu,\n",
    "    max_epochs=args.num_train_epochs,\n",
    " #   early_stop_callback=False,\n",
    "    precision= 16 if args.fp_16 else 32,\n",
    "    amp_level=args.opt_level,\n",
    "    gradient_clip_val=args.max_grad_norm,\n",
    "    checkpoint_callback=checkpoint_callback,\n",
    "    callbacks=[LoggingCallback()],\n",
    ")\n",
    "\n",
    "def get_dataset(tokenizer, type_path, args):\n",
    "    return ParaphraseDataset(tokenizer=tokenizer, data_dir=args.data_dir, type_path=type_path,  max_len=args.max_seq_length)\n",
    "\n",
    "print (\"Initialize model\")\n",
    "model = T5FineTuner(args)\n",
    "\n",
    "trainer = pl.Trainer(**train_params)\n",
    "\n",
    "print (\" Training model\")\n",
    "trainer.fit(model)\n",
    "\n",
    "print (\"training finished\")\n",
    "\n",
    "print (\"Saving model\")\n",
    "model.model.save_pretrained('t5_paraphrase')\n",
    "\n",
    "print (\"Model saved\")\n",
    "\n",
    "!cp \"/content/t5_paraphrase/\" -a \"/content/drive/My Drive/\"\n",
    "!cp \"/content/lightning_logs/\" -a \"/content/drive/My Drive/\"\n",
    "print (\"Copied the final folder to Google Drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
